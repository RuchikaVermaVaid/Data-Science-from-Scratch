{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "from Vector_operations_on_data import distance,add,scalar_multiply,vector_mean #Importing functions from the previous code\n",
    "from typing import Callable\n",
    "\n",
    "Vector = List[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##########################################################################################################\n",
    "##########################    Calling function if f is a function of one variable   ######################\n",
    "##########################################################################################################\n",
    "\"\"\"\n",
    "# Callable function : square\n",
    "def square(x:float) -> float:\n",
    "    return x*x\n",
    "\n",
    "# Derivative of a square function\n",
    "def derivative(x:float) -> float:\n",
    "    return 2*x\n",
    "\n",
    "# Estimating solution of a derivative function\n",
    "def difference_quotient(f: Callable[[float],float], x: float,h: float)-> float:\n",
    "    return (f(x+h) -f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "[-19.998999999984335, -17.998999999988996, -15.999000000007868, -13.999000000005424, -11.99900000000298, -9.999000000004088, -7.998999999999867, -5.998999999999199, -3.9989999999994197, -1.998999999999973, 0.001, 2.0009999999996975, 4.000999999999699, 6.000999999999479, 8.0010000000037, 10.001000000002591, 12.001000000005035, 14.00100000000748, 16.000999999988608, 18.000999999983947, 20.000999999993496]\n"
     ]
    }
   ],
   "source": [
    "xs = [x for x in range(-10,11)]\n",
    "actuals = [derivative(x) for x in xs] # Get actual values in xs using derivative function\n",
    "print(xs)\n",
    "print(actuals)\n",
    "\n",
    "# Get estimates of the derivative function using difference_quotient\n",
    "estimates = [difference_quotient(square, x,h = 0.001) for x in xs]\n",
    "print(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting to see if actual derivative values of a square function matches with the estimated values\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\" Actual derivatives vs. Estimates\")\n",
    "plt.plot(xs,actuals,'rx',label = 'Actual')\n",
    "plt.plot(xs,estimates,'b+',label = 'Estimate')\n",
    "plt.legend(loc = 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##########################################################################################################\n",
    "##########################    Calling function if f is a function of many variables ######################\n",
    "##########################################################################################################\n",
    "\"\"\"\n",
    "# Partial derivatives, f is a function of many variables\n",
    "def partial_difference_quotient(f: Callable[[Vector],float],v:Vector, i:int,h:float)-> float:\n",
    "    #Returns the ith partial difference quotient of f at v\n",
    "    w = [v_j + (h if j ==i else 0) \n",
    "        for j,v_j in enumerate(v)]\n",
    "    \n",
    "    return (f(w)-f(v))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector],float],v:Vector,h:float = 0.0001)-> float:\n",
    "    return [partial_difference_quotient(f,v,i,h)\n",
    "           for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step (v:Vector, gradient:Vector,step_size:float) -> Vector:\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size,gradient)\n",
    "    return add(v,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v:Vector)-> Vector:\n",
    "    return [2*v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.166334200246083, 1.3875788778188547, 2.8171214624252365]\n",
      "1 [1.1430075162411613, 1.3598273002624777, 2.760779033176732]\n",
      "2 [1.1201473659163381, 1.3326307542572282, 2.7055634525131973]\n",
      "3 [1.0977444185980114, 1.3059781391720837, 2.651452183462933]\n",
      "4 [1.0757895302260512, 1.279858576388642, 2.5984231397936743]\n",
      "5 [1.05427373962153, 1.2542614048608693, 2.5464546769978007]\n",
      "6 [1.0331882648290995, 1.229176176763652, 2.4955255834578445]\n",
      "7 [1.0125244995325176, 1.204592653228379, 2.4456150717886875]\n",
      "8 [0.9922740095418672, 1.1805008001638115, 2.396702770352914]\n",
      "9 [0.9724285293510299, 1.1568907841605354, 2.3487687149458556]\n",
      "10 [0.9529799587640093, 1.1337529684773247, 2.3017933406469386]\n",
      "989 [2.4513869536651658e-09, 2.916396310378981e-09, 5.920991426318714e-09]\n",
      "990 [2.4023592145918623e-09, 2.858068384171401e-09, 5.80257159779234e-09]\n",
      "991 [2.354312030300025e-09, 2.800907016487973e-09, 5.6865201658364936e-09]\n",
      "992 [2.3072257896940244e-09, 2.7448888761582134e-09, 5.572789762519764e-09]\n",
      "993 [2.261081273900144e-09, 2.689991098635049e-09, 5.461333967269369e-09]\n",
      "994 [2.215859648422141e-09, 2.6361912766623482e-09, 5.352107287923981e-09]\n",
      "995 [2.1715424554536982e-09, 2.583467451129101e-09, 5.245065142165501e-09]\n",
      "996 [2.1281116063446243e-09, 2.531798102106519e-09, 5.140163839322191e-09]\n",
      "997 [2.085549374217732e-09, 2.4811621400643887e-09, 5.0373605625357475e-09]\n",
      "998 [2.0438383867333772e-09, 2.431538897263101e-09, 4.936613351285032e-09]\n",
      "999 [2.0029616189987095e-09, 2.382908119317839e-09, 4.837881084259331e-09]\n"
     ]
    }
   ],
   "source": [
    "# Pick a random starting point to find minimum in a 3D vector\n",
    "# Take tiny steps in the opposite of gradient until we reach a point where gradient is very small\n",
    "v = [random.uniform(-10,10) for i in range(3)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)\n",
    "    v = gradient_step(v,grad,-0.01)\n",
    "    print(epoch,v)\n",
    "    \n",
    "assert distance(v,[0,0,0])<0.001 # v should be close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-50, -995), (-49, -975), (-48, -955), (-47, -935), (-46, -915), (-45, -895), (-44, -875), (-43, -855), (-42, -835), (-41, -815), (-40, -795), (-39, -775), (-38, -755), (-37, -735), (-36, -715), (-35, -695), (-34, -675), (-33, -655), (-32, -635), (-31, -615), (-30, -595), (-29, -575), (-28, -555), (-27, -535), (-26, -515), (-25, -495), (-24, -475), (-23, -455), (-22, -435), (-21, -415), (-20, -395), (-19, -375), (-18, -355), (-17, -335), (-16, -315), (-15, -295), (-14, -275), (-13, -255), (-12, -235), (-11, -215), (-10, -195), (-9, -175), (-8, -155), (-7, -135), (-6, -115), (-5, -95), (-4, -75), (-3, -55), (-2, -35), (-1, -15), (0, 5), (1, 25), (2, 45), (3, 65), (4, 85), (5, 105), (6, 125), (7, 145), (8, 165), (9, 185), (10, 205), (11, 225), (12, 245), (13, 265), (14, 285), (15, 305), (16, 325), (17, 345), (18, 365), (19, 385), (20, 405), (21, 425), (22, 445), (23, 465), (24, 485), (25, 505), (26, 525), (27, 545), (28, 565), (29, 585), (30, 605), (31, 625), (32, 645), (33, 665), (34, 685), (35, 705), (36, 725), (37, 745), (38, 765), (39, 785), (40, 805), (41, 825), (42, 845), (43, 865), (44, 885), (45, 905), (46, 925), (47, 945), (48, 965), (49, 985)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "##########################################################################################################\n",
    "##########################            Use gradient descent to fit models            ######################\n",
    "##########################                   Batch gradient descent                  ######################\n",
    "##########################################################################################################\n",
    "\"\"\"\n",
    "# x ranges from -50 to 49, y is always 20*x+5\n",
    "inputs = [(x,20*x+5) for x in range(-50,50)]\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x:float,y:float,theta:Vector)-> Vector:\n",
    "    slope,intercept = theta\n",
    "    predicted = slope*x+intercept # The prediction of the model\n",
    "    error = (predicted - y) #error is predicted - actual\n",
    "    squared_error = error**2 #Minime the squared error\n",
    "    grad = [2*error*x,2*error]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.4215732004032178, -0.5297549553127789]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with random values for slope and intercept\n",
    "theta = [random.uniform(-1,1), random.uniform(-1,1)]\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [33.61565956971364, -0.5391170186025565]\n",
      "1 [10.912815949982392, -0.5144231249956376]\n",
      "2 [26.05563733823675, -0.5124814627956639]\n",
      "3 [15.955377413933293, -0.4954008625318358]\n",
      "4 [22.692267864043963, -0.4884546833928388]\n",
      "5 [18.198768879999285, -0.47478550616200915]\n",
      "6 [21.195946371534315, -0.4656371662696859]\n",
      "7 [19.196838133020343, -0.45350994556561225]\n",
      "8 [20.530255455329865, -0.44340608754146066]\n",
      "9 [19.640876205207437, -0.4319890199110479]\n",
      "10 [20.234103582106727, -0.4214841656660184]\n",
      "4991 [19.999999847517373, 4.999746116334068]\n",
      "4992 [19.999999847822245, 4.999746623948917]\n",
      "4993 [19.99999984812651, 4.999747130548841]\n",
      "4994 [19.999999848430168, 4.99974763613587]\n",
      "4995 [19.999999848733214, 4.999748140712029]\n",
      "4996 [19.999999849035657, 4.999748644279338]\n",
      "4997 [19.999999849337495, 4.9997491468398145]\n",
      "4998 [19.99999984963873, 4.999749648395472]\n",
      "4999 [19.999999849939364, 4.99975014894832]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "for epoch in range(5000):\n",
    "    #Compute the mean of the gradients\n",
    "    grad = vector_mean([linear_gradient(x,y,theta) for x,y in inputs])\n",
    "    # Take a step in that direction\n",
    "    theta = gradient_step(theta, grad,-learning_rate)\n",
    "    print(epoch, theta)\n",
    "    \n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1 # Slope should be around 20\n",
    "assert 4.9 < intercept < 5.1 # Intercept should be around 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##########################################################################################################\n",
    "################# Split data into mini-batches and use gradient descent to fit models ####################\n",
    "##########################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypeVar, List, Iterator\n",
    "T = TypeVar('T')\n",
    "\n",
    "def minibatches(dataset:List[T],\n",
    "               batch_size = int,\n",
    "               shuffle:bool = True) -> Iterator[List[T]]:\n",
    "    # Generate minibatches of batchsize from the datset\n",
    "    #Index starts from 0\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    \n",
    "    if shuffle:random.shuffle(batch_starts) #Shuffle the batches\n",
    "    \n",
    "    for start in batch_starts:\n",
    "        end = start+batch_size\n",
    "        yield dataset[start:end]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [18.517363091241375, 1.7808560846726924]\n",
      "1 [19.699215613372097, 1.7732172258020102]\n",
      "2 [19.826391913707262, 1.7980870181560493]\n",
      "3 [19.48892269786346, 1.8800017146711103]\n",
      "4 [20.04733763448632, 1.9420861785308678]\n",
      "5 [20.007296853146265, 2.0827023199173986]\n",
      "6 [18.672826497211233, 2.1625850862728107]\n",
      "7 [19.623924592910804, 2.1459358736419345]\n",
      "8 [19.96905351118571, 2.2569009292039874]\n",
      "9 [20.09316990959831, 2.292167887918438]\n",
      "10 [20.110381781360438, 2.3024839443778062]\n",
      "990 [19.999999759400822, 4.999997598267602]\n",
      "991 [19.99999962804993, 4.999997677823221]\n",
      "992 [19.999999883415278, 4.9999978945921]\n",
      "993 [20.00000013806604, 4.999997903377837]\n",
      "994 [19.99999988532921, 4.999997910890041]\n",
      "995 [20.000000063432054, 4.9999979561903745]\n",
      "996 [19.999999515488472, 4.999997983223836]\n",
      "997 [19.99999998365643, 4.999997997537947]\n",
      "998 [20.00000001870291, 4.999998009899814]\n",
      "999 [20.000000097181715, 4.999998018242733]\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1,1), random.uniform(-1,1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size = 20):\n",
    "        grad = vector_mean([linear_gradient(x,y,theta) for x,y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print (epoch,theta)\n",
    "    \n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1 # Slope should be around 20\n",
    "assert 4.9 < intercept < 5.1 # Intercept should be around 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [20.102302671939782, -0.09181704665302065]\n",
      "1 [20.09791283387859, 0.12664107806112238]\n",
      "2 [20.09371200518361, 0.33572651405147924]\n",
      "3 [20.089691391575222, 0.535841397726443]\n",
      "4 [20.08584330286927, 0.7273705997826726]\n",
      "5 [20.082160308586428, 0.910682478672682]\n",
      "6 [20.078635328405888, 1.0861295884243798]\n",
      "7 [20.07526155745144, 1.254049356687741]\n",
      "8 [20.072032565816414, 1.4147647349173262]\n",
      "9 [20.06894209317885, 1.568584818791328]\n",
      "10 [20.06598421945285, 1.7158054422036888]\n",
      "990 [20.000000002709324, 5.000000000324199]\n",
      "991 [20.000000002326704, 5.000000000408773]\n",
      "992 [20.00000001005339, 5.000000000639604]\n",
      "993 [20.00000000019266, 5.000000000805643]\n",
      "994 [19.99999999263345, 5.000000000625624]\n",
      "995 [19.999999986186552, 5.000000000180019]\n",
      "996 [19.99999999802444, 4.999999999872501]\n",
      "997 [19.999999993008544, 4.999999999698645]\n",
      "998 [19.999999991369503, 4.99999999940447]\n",
      "999 [20.000000003838124, 4.9999999993452455]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "##########################################################################################################\n",
    "####    Use Stochastic gradient descent in which you take gradient steps based on one training example ###\n",
    "##########################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "theta = [random.uniform(-1,1), random.uniform(-1,1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for x,y in inputs:\n",
    "        grad = linear_gradient(x,y,theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print (epoch,theta)\n",
    "    \n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1 # Slope should be around 20\n",
    "assert 4.9 < intercept < 5.1 # Intercept should be around 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
